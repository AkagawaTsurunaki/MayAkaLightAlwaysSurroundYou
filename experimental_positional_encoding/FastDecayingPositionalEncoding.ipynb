{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Fast Decaying Positional Encoding\n",
    "\n",
    "$$ {PE}_{pos, 2i} = \\frac{2i}{d_{\\text{model}}} \\cdot \\frac{\\sin(pos)}{\\sqrt{pos}}$$\n",
    "$$ {PE}_{pos, 2i+1} = \\frac{2i+1}{d_{\\text{model}}} \\cdot \\frac{\\cos(pos)}{\\sqrt{pos}}$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "825b481b979944dc"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is torch.Size([1, 200, 512])\n",
      "tensor([[[ 9.9683e-04,  6.4006e-04,  2.9905e-03,  ...,  3.2579e-01,\n",
      "           5.0938e-01,  3.2707e-01],\n",
      "         [ 6.5334e-04, -2.9901e-04,  1.9600e-03,  ..., -1.5219e-01,\n",
      "           3.3386e-01, -1.5279e-01],\n",
      "         [ 6.1500e-05, -4.3144e-04,  1.8450e-04,  ..., -2.1960e-01,\n",
      "           3.1427e-02, -2.2047e-01],\n",
      "         ...,\n",
      "         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -1.0089e-43,\n",
      "          -8.4078e-45, -1.0089e-43],\n",
      "         [-0.0000e+00, -0.0000e+00, -0.0000e+00,  ..., -2.9427e-44,\n",
      "          -5.4651e-44, -2.9427e-44],\n",
      "         [-0.0000e+00,  0.0000e+00, -0.0000e+00,  ...,  1.8217e-44,\n",
      "          -3.2230e-44,  1.8217e-44]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "def has_duplicate_rows(matrix):\n",
    "    \"\"\"\n",
    "    To check if there are same row in the given matrix\n",
    "    :param matrix: torch.Tensor\n",
    "    :return: True if given matrix has duplicate rows, False if not.\n",
    "    \"\"\"\n",
    "    unique_rows = torch.unique(matrix, dim=1)\n",
    "    return unique_rows.shape[1] < matrix.shape[1]\n",
    "\n",
    "\n",
    "class FastDecayingPositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Fast Decaying Positional Encoding\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int = 512, dropout: float = 0, max_len: int = 200):\n",
    "        super(FastDecayingPositionalEncoding, self).__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.unsqueeze(torch.arange(start=1, end=max_len + 1), dim=1)  # Shape is torch.Size([max_len, 1])\n",
    "\n",
    "        term = torch.exp(\n",
    "            torch.log(torch.arange(start=1, end=d_model + 1, step=2)) -\n",
    "            torch.log(torch.Tensor([d_model])) -\n",
    "            (0.5 * position)\n",
    "        )  # Shape is torch.Size([max_len, d_model / 2])\n",
    "\n",
    "        pe[:, 0::2] = term * torch.sin(position)  # pe_pos_2i\n",
    "        pe[:, 1::2] = term * torch.cos(position)  # pe_pos_2i+1\n",
    "\n",
    "        pe = torch.unsqueeze(pe, dim=0)  # Shape is torch.Size([1, max_len, d_model])\n",
    "\n",
    "        if has_duplicate_rows(pe):\n",
    "            raise Warning(\n",
    "                'The positional encoding matrix contains invalid encoding rows, which may result in the loss of positional information. Please reduce the value of the \"max_len\" parameter.')\n",
    "\n",
    "        self.register_buffer(name='pe', tensor=pe)  # Buffer\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x.shape = torch.Size([max_len, d_model])\n",
    "        x = x + self.pe[:, :x.size(1)].requires_grad_(False)  # Prevent from calculating gradients.\n",
    "        return self.dropout(x)  # Stochastically dropout elements\n",
    "\n",
    "\n",
    "pe = FastDecayingPositionalEncoding().pe\n",
    "print(f'Shape is {pe.shape}')\n",
    "print(pe)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T11:59:54.257391400Z",
     "start_time": "2023-10-13T11:59:54.243821900Z"
    }
   },
   "id": "943721a4d05d546e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
